{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import statistics \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from math import sqrt\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in dataset:\n",
    "data = np.loadtxt(\"zipcombo.dat\")\n",
    "\n",
    "# Shuffle rows of data matrix\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Slit data into 80% train set and 20% test set\n",
    "test_data = data[:int(np.round(0.2*9298)),:]\n",
    "train_data = data[int(np.round(0.2*9298)):,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Decision Tree Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split and evaluate each split using Gini index\n",
    "def test_split(index, dataset):\n",
    "    total_left = 0 \n",
    "    total_right = dataset.shape[0] \n",
    "    left_proportions = np.zeros((1,10)) \n",
    "    right_proportions = np.zeros((1,10))\n",
    "    gini_index = np.inf \n",
    "    \n",
    "    # sort chosen feature in order, from smallest to largest value\n",
    "    indices = np.argsort(dataset[:,index]) \n",
    "    order_dataset = dataset[indices,0:index+1:index]\n",
    "    # to get number of repeated feature values\n",
    "    feature_values, feature_counts = np.unique(order_dataset[:,1], return_counts = True) \n",
    "   \n",
    "    # place everything in the 'right' cluster first, i.e. record how many times each label occured in the right_proporions vector \n",
    "    for i in range(order_dataset.shape[0]):\n",
    "        right_proportions[0,int(order_dataset[i,0])] += 1 \n",
    "\n",
    "    row_index = 0\n",
    "    \n",
    "    # Go through feature column, one-by-one record corresponding label in left_proportions vector, remove from right_proportions vector \n",
    "    for feature_value, feature_count in zip(feature_values[:-1], feature_counts[:-1]):\n",
    "        # to account for repeated feature values\n",
    "        for j in range(feature_count):\n",
    "            left_proportions[0,int(order_dataset[row_index,0])] += 1\n",
    "            total_left += 1\n",
    "            right_proportions[0,int(order_dataset[row_index,0])] -= 1\n",
    "            total_right -= 1\n",
    "            row_index += 1\n",
    "        \n",
    "        # to avoid dividing by 0    \n",
    "        if total_left == 0 or total_right == 0:\n",
    "            continue\n",
    "        \n",
    "        # find Gini index after each split ('movement of label' from right_proportions to left_proportions)    \n",
    "        p_left = (left_proportions / total_left).dot((left_proportions / total_left).T)\n",
    "        p_right = (right_proportions / total_right).dot((right_proportions / total_right).T)\n",
    "        \n",
    "        gini_left = (1.0 - p_left) * (total_left / dataset.shape[0])\n",
    "        gini_right = (1.0 - p_right) * (total_right / dataset.shape[0])\n",
    "        gini = gini_left + gini_right\n",
    "        \n",
    "        # record row index and index value where the split resulted in the lowest Gini index\n",
    "        if gini < gini_index:\n",
    "            gini_index = gini\n",
    "            global best_split_value\n",
    "            best_split_value = order_dataset[row_index,1]\n",
    "            global best_row_index\n",
    "            best_row_index = row_index\n",
    "     \n",
    "    return order_dataset, left_proportions, right_proportions, gini_index, index, best_split_value, best_row_index\n",
    "\n",
    "# Function to get two split datasets at best split position (split which yeilds lowest Gini index)\n",
    "def get_split(dataset, n_features):\n",
    "    b_score = np.inf\n",
    "    # sample features without replacement \n",
    "    features_array = np.random.choice(np.arange(1,len(dataset[0])), size = (n_features,), replace = False)\n",
    "    features = features_array.tolist()\n",
    "    \n",
    "    for index in features:\n",
    "        ordered, left_prop, right_prop, gini_index, index, best_split, best_row_index = test_split(index, dataset) \n",
    "        \n",
    "        # record feature index and feature value for best split (with lowest Gini Index)\n",
    "        if gini_index < b_score:\n",
    "            global b_index, b_row_index, b_value\n",
    "            b_index, b_value, b_score, b_row_index, b_left, b_right = index, best_split, gini_index, best_row_index, left_prop, right_prop \n",
    "    \n",
    "    # Get seperate datasets 'left' and 'right' where split occurs  \n",
    "    indices = np.argsort(dataset[:,b_index])\n",
    "    order_dataset = dataset[indices,0:]\n",
    "    left = order_dataset[:b_row_index]\n",
    "    right = order_dataset[b_row_index:]\n",
    "    \n",
    "    return {'index':b_index, 'value':b_value, 'left_group':left, 'right_group':right} \n",
    "\n",
    "# Get majority vote for a group of data points (used to evaluate prediction of leaves)\n",
    "def to_terminal(group):\n",
    "    unique_vals = np.unique(group[:,0], return_counts = True)\n",
    "    \n",
    "    return unique_vals[0][np.argmax(unique_vals[1])] \n",
    "\n",
    "# Create function to either keep splitting sub-groups ('left' and 'right'), or evaluate prediction of sub-groups\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['left_group'], node['right_group']\n",
    "    del node['left_group'] \n",
    "    del node['right_group'] \n",
    "    \n",
    "    # in case no split exists\n",
    "    if right.shape[0] == 0:\n",
    "        node['left'] = node['right'] = to_terminal(np.vstack((left,right))) \n",
    "        return\n",
    "    \n",
    "    # if maximum tree depth has been reached\n",
    "    if depth >= max_depth: \n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right) \n",
    "        return\n",
    "    \n",
    "    # if minimum number of data points in 'left' leaf is reached evaluate, or if not split further \n",
    "    if left.shape[0] <= min_size: \n",
    "        node['left'] = to_terminal(left) \n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features) # if not split further\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "    \n",
    "    # if minimum number of data points in 'right' leaf is reached evaluate, or if not split further\n",
    "    if right.shape[0] <= min_size: # do same for right node\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    "\n",
    "# Function to build a single decision tree\n",
    "def build_tree(dataset, max_depth, min_size, n_features):\n",
    "    root = get_split(dataset, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1) # greedy partition\n",
    "    return root\n",
    "\n",
    "# Function to make a prediction on a single decision tree\n",
    "def predict(dataset, node, row):\n",
    "    if dataset[row,[node['index']]] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(dataset, node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(dataset, node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample with replacements from original dataset to create ensemble of trees\n",
    "def subsample(dataset):\n",
    "    sample = np.zeros((dataset.shape[0],dataset.shape[1]))\n",
    "    sample_idx = np.random.choice(np.arange(dataset.shape[0]), size = (dataset.shape[0],), replace = True)\n",
    "    s_row_index = 0\n",
    "    for i in sample_idx.tolist():\n",
    "        sample[s_row_index] = dataset[i]\n",
    "        s_row_index += 1\n",
    "        \n",
    "    return sample\n",
    "\n",
    "# create ensemble of random forests:\n",
    "def create_ensemble(dataset, test, max_depth, min_size, n_features, n_trees):\n",
    "    predictions_test = np.zeros((1,test.shape[1]))\n",
    "    predictions_train = np.zeros((1,dataset.shape[1]))\n",
    "    for i in range(n_trees):\n",
    "        sample = subsample(dataset)\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "        \n",
    "        prediction_test = [predict(test, tree, row) for row in range(test.shape[0])]\n",
    "        prediction_train = [predict(dataset, tree, row) for row in range(dataset.shape[0])]\n",
    "        \n",
    "        if i == 0:\n",
    "            predictions_test = np.array(prediction_test)\n",
    "            predictions_train = np.array(prediction_train)\n",
    "        else:\n",
    "            predictions_test = np.vstack((predictions_test, np.array(prediction_test)))\n",
    "            predictions_train = np.vstack((predictions_train, np.array(prediction_train)))\n",
    "    # Get overall prediction:\n",
    "    final_predictions_test = np.zeros((1,test.shape[0]))\n",
    "    final_predictions_train = np.zeros((1,dataset.shape[0]))\n",
    "    # get majority vote for each prediction in ensemble (prediction on test dataset) \n",
    "    for i in range(test.shape[0]):\n",
    "        all_values_test = []\n",
    "        for j in range(n_trees):\n",
    "            all_values_test.append(predictions_test[j,i])\n",
    "        values_test = Counter(all_values_test)\n",
    "        value_test, count_test = (values_test).most_common()[0]\n",
    "        final_predictions_test[:,i] = value_test\n",
    "        \n",
    "    # get majority vote for each prediction in ensemble (prediction on train dataset) \n",
    "    for i in range(dataset.shape[0]):\n",
    "        all_values_train = []\n",
    "        for j in range(n_trees):\n",
    "            all_values_train.append(predictions_train[j,i])\n",
    "        values_train = Counter(all_values_train)\n",
    "        value_train, count_train = (values_train).most_common()[0]\n",
    "        final_predictions_train[:,i] = value_train\n",
    "    \n",
    "    return final_predictions_test, final_predictions_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run model (to test):\n",
    "n_trees = 100\n",
    "dataset = train_data\n",
    "\n",
    "max_depth = 10\n",
    "min_size = 5\n",
    "n_features = int(sqrt(len(dataset[1])-1))\n",
    "\n",
    "# Record how long model takes\n",
    "start = time.time()\n",
    "start_m = time.time()\n",
    "final_predictions_test, final_predictions_train = create_ensemble(dataset, test_data, max_depth, min_size, n_features, n_trees)\n",
    "print('time take was:', time.time() - start_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if accuracy values are reasonable and match that of Sklearn\n",
    "accuracy_train = (np.sum(final_predictions_train == train_data[:,0]) / train_data.shape[0]) * 100\n",
    "print(accuracy_train)\n",
    "\n",
    "accuracy_test = (np.sum(final_predictions_test == test_data[:,0]) / test_data.shape[0]) * 100\n",
    "print(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1 Repeated with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment to find no. of trees to vary in cross-val (also experiment other parameters to see what variables have the biggest inpact on performance):\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "n_features = int(sqrt(len(dataset[1])-1))\n",
    "for n_trees in [3, 10, 30, 50, 70, 100]:\n",
    "    print('number of trees', n_trees)\n",
    "    final_predictions_test, final_predictions_train = create_ensemble(train_data, test_data, max_depth, min_size, n_features, n_trees)\n",
    "    accuracy_train = np.sum(final_predictions_train == train_data[:,0]) / train_data.shape[0] * 100\n",
    "    print('train accuracy:', accuracy_train)\n",
    "    \n",
    "    accuracy_test = np.sum(final_predictions_test == test_data[:,0]) / test_data.shape[0] * 100\n",
    "    print('test accuracy:', accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 20 runs for the various hyperparameters we selected:\n",
    "# Set this up but code takes too long (two days to get what I want) . Code is not set up to run multiple decision trees in parallel like the way 'Sklearn' does\n",
    "# Since code too took long, I initially thought to fix the number of trees to 30 and just vary the minumim number of samples in each leaf (but this parameter was found to not make a big enough impact on the test error)\n",
    "# Therefore, the Sklearn was set-up so that we could also vary the number of trees along with the minimum number of samples in good time\n",
    "\n",
    "n_trees = 30\n",
    "for min_size in [1, 3, 5, 7, 9, 11, 13]:\n",
    "    print('min size:',min_size)\n",
    "    \n",
    "    train_total_error = []\n",
    "    test_total_error = [] \n",
    "    \n",
    "    for k in range(20):\n",
    "        print('run:',k)\n",
    "        # shuffle data before each run\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        # Slit data into 80% train set and 20% test set\n",
    "        test_data = data[:int(np.round(0.2*9298)),:]\n",
    "        train_data = data[int(np.round(0.2*9298)):,:]\n",
    "\n",
    "        final_predictions_test, final_predictions_train = create_ensemble(train_data, test_data, max_depth, min_size, n_features, n_trees)\n",
    "        accuracy_train = np.sum(final_predictions_train == train_data[:,0]) / train_data.shape[0] * 100\n",
    "        print('train accuracy:', accuracy_train)\n",
    "\n",
    "        accuracy_test = np.sum(final_predictions_test == test_data[:,0]) / test_data.shape[0] * 100\n",
    "        print('test accuracy:', accuracy_test)\n",
    "\n",
    "        train_total_error.append(100 - accuracy_train)\n",
    "        test_total_error.append(100 - accuracy_test)\n",
    "\n",
    "    print('min size is', min_size, ' mean of total train error ', np.mean(train_total_error) , 'standard deviation', (statistics.stdev(train_total_error)), ' and test mistakes ', np.mean(test_total_error), 'standard deviation', (statistics.stdev(test_total_error)), ) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
